<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>卷积神经网基础串讲 | ieblYang</title>
        <meta name="Description" content="基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲"><meta property="og:title" content="卷积神经网基础串讲" />
<meta property="og:description" content="基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ieblyang.tech/face03/" />
<meta property="article:published_time" content="2020-03-12T12:43:40+08:00" />
<meta property="article:modified_time" content="2020-03-12T12:43:40+08:00" /><meta property="og:site_name" content="ieblYang" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="卷积神经网基础串讲"/>
<meta name="twitter:description" content="基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲"/>
<meta name="theme-color" content="#ffffff">
<meta name="msapplication-TileColor" content="#da532c">
<link rel="canonical" href="https://ieblyang.tech/face03/" />
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="prev" href="https://ieblyang.tech/face02/" /><link rel="next" href="https://ieblyang.tech/face04/" /><link rel="stylesheet" href="/lib/fontawesome-free/all.min.9679f10076c3db0a03c8c8a5128dba53.css" integrity="md5-lnnxAHbD2woDyMilEo26Uw=="><link rel="stylesheet" href="/lib/animate/animate.min.bc1a6a99c43f5ccc97d2d350bde13f74.css" integrity="md5-vBpqmcQ/XMyX0tNQveE/dA=="><link rel="stylesheet" href="/css/style.min.ae21e349d7dcddfd942a94491d8158d5.css" integrity="md5-riHjSdfc3f2UKpRJHYFY1Q=="><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "卷积神经网基础串讲",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/ieblyang.tech\/face03\/"
        },"image": {
                "@type": "ImageObject",
                "url": "https:\/\/ieblyang.tech\/cover.png",
                "width":  800 ,
                "height":  600 
            },"genre": "posts","keywords": "人脸识别, 卷积神经网","wordcount":  4345 ,
        "url": "https:\/\/ieblyang.tech\/face03\/","datePublished": "2020-03-12","dateModified": "2020-03-12","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
                "@type": "Organization",
                "name": "ieblYang",
                "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/ieblyang.tech\/logo.png",
                "width":  127 ,
                "height":  40 
                }
            },"author": {
                "@type": "Person",
                "name": "ieblYang"
            },"description": "基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲"
    }
    </script></head>
    <body><script>
            if (!window.localStorage || !window.localStorage.getItem('theme')) {window.isDark = window.matchMedia('(prefers-color-scheme: dark)').matches;} else {
                window.isDark = (window.localStorage && window.localStorage.getItem('theme')) === 'dark';
            }
            window.isDark && document.body.classList.add('dark-theme');
        </script><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/">ieblYang</a>
        </div>
        <div class="menu"><a class="menu-item" href="/posts/" rel="noopener noreffer">文章</a><a class="menu-item" href="/tags/" rel="noopener noreffer">标签</a><a class="menu-item" href="/categories/" rel="noopener noreffer">分类</a><a class="menu-item" href="/categories/documentation/" rel="noopener noreffer">文档</a><a class="menu-item" href="/about/" rel="noopener noreffer">关于</a><a class="menu-item" href="https://github.com/ieblYang" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><span class="menu-item">|</span>
            <a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/en/face03/">English</option><option value="/face03/" selected>简体中文</option></select>
                </a></div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-wrapper">
        <div class="header-container">
            <div class="header-title">
                <a href="/">ieblYang</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="" rel="noopener noreffer">文章</a><a class="menu-item" href="/tags/" title="" rel="noopener noreffer">标签</a><a class="menu-item" href="/categories/" title="" rel="noopener noreffer">分类</a><a class="menu-item" href="/categories/documentation/" title="" rel="noopener noreffer">文档</a><a class="menu-item" href="/about/" title="" rel="noopener noreffer">关于</a><a class="menu-item" href="https://github.com/ieblYang" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/en/face03/">English</option><option value="/face03/" selected>简体中文</option></select>
                </a></div>
    </div>
</header>

<script>
    window.desktopHeaderMode = "fixed";
    window.mobileHeaderMode = "auto";
</script>
<main class="main">
                <div class="container"><article class="page single"><h1 class="single-title animated flipInX">卷积神经网基础串讲</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://ieblyang.github.io/" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>ieblYang</a>
</span>&nbsp;
                    <span class="post-category">收录于<a href="/categories/face-recognition">
                                <i class="far fa-folder fa-fw"></i>Face recognition
                            </a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i><time datetime=2020-03-12>2020-03-12</time>&nbsp;
                <i class="fas fa-pencil-alt fa-fw"></i>约 4345 字&nbsp;
                <i class="far fa-clock fa-fw"></i>预计阅读 9 分钟&nbsp;<span id="/face03/" class="leancloud_visitors" data-flag-title="卷积神经网基础串讲">
                        <i class="far fa-eye fa-fw"></i><span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="featured-image"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/featured-image.png, /images/face/featured-image.png 1.5x, /images/face/featured-image.png 2x"
        data-src="/images/face/featured-image.png"
        alt="基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲"
        title="基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲" /></div><div class="toc" id="toc-auto">
                <h2 class="toc-title">目录</h2>
                <div class="toc-content" id="toc-content-auto"></div>
            </div>
            <div class="toc" id="toc-static">
                <details>
                    <summary>
                        <div class="toc-title">
                            <span>目录</span>
                            <span><i class="details icon fas fa-angle-down"></i></span>
                        </div>
                    </summary>
                    <div class="toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-卷积神经网发展历程与基本概念">1 卷积神经网发展历程与基本概念</a>
      <ul>
        <li><a href="#11-什么是卷积神经网">1.1 什么是卷积神经网</a></li>
      </ul>
    </li>
    <li><a href="#2-卷积神经网络的重要组成单元">2 卷积神经网络的重要组成单元</a>
      <ul>
        <li><a href="#21-卷积">2.1 卷积</a>
          <ul>
            <li><a href="#211-基本定义">2.1.1 基本定义</a></li>
            <li><a href="#212-卷集中的重要参数">2.1.2 卷集中的重要参数</a>
              <ul>
                <li><a href="#1-卷积核">1. 卷积核</a></li>
                <li><a href="#2-卷积">2. 卷积</a></li>
                <li><a href="#3-卷积核与感受野">3. 卷积核与感受野</a></li>
                <li><a href="#4-步长">4. 步长</a></li>
                <li><a href="#5-pad">5. Pad</a></li>
              </ul>
            </li>
            <li><a href="#213-卷积的定义与使用caffe">2.1.3 卷积的定义与使用（caffe）</a></li>
            <li><a href="#214-卷积的定义与使用tensorflow">2.1.4 卷积的定义与使用（TensorFlow）</a></li>
          </ul>
        </li>
        <li><a href="#22-池化">2.2 池化</a>
          <ul>
            <li><a href="#221-基本定义">2.2.1 基本定义</a></li>
            <li><a href="#222-常见的池化策略">2.2.2 常见的池化策略</a></li>
          </ul>
        </li>
        <li><a href="#23-激活">2.3 激活</a>
          <ul>
            <li><a href="#231-基本定义">2.3.1 基本定义</a></li>
            <li><a href="#232-sigmoid">2.3.2 Sigmoid</a></li>
            <li><a href="#233-tanh">2.3.3 Tanh</a></li>
            <li><a href="#234-relu">2.3.4 ReLU</a></li>
          </ul>
        </li>
        <li><a href="#24-bnbatchnorm">2.4 BN（BatchNorm）</a>
          <ul>
            <li><a href="#241-基本概念">2.4.1 基本概念</a></li>
            <li><a href="#242-batchnorm层优点">2.4.2 BatchNorm层优点</a></li>
            <li><a href="#243-batchnorm层使用">2.4.3 BatchNorm层使用</a></li>
          </ul>
        </li>
        <li><a href="#25-全连接层fully-connected-layer">2.5 全连接层（Fully Connected Layer）</a></li>
        <li><a href="#26-dropout层">2.6 Dropout层</a></li>
        <li><a href="#27-损失层loss">2.7 损失层（LOSS）</a>
          <ul>
            <li><a href="#271-损失函数">2.7.1 损失函数</a></li>
            <li><a href="#272-损失层">2.7.2 损失层</a></li>
            <li><a href="#273-交叉熵损失">2.7.3 交叉熵损失</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#3-常见的卷积神经网结构">3 常见的卷积神经网结构</a>
      <ul>
        <li><a href="#31-lenet">3.1 LeNet</a></li>
        <li><a href="#32-alexnet">3.2 AlexNet</a>
          <ul>
            <li><a href="#321-概念">3.2.1 概念</a></li>
            <li><a href="#322-alexnet的特点">3.2.2 AlexNet的特点</a></li>
            <li><a href="#323-alexnet的意义">3.2.3 AlexNet的意义</a></li>
          </ul>
        </li>
        <li><a href="#33-zfnet">3.3 ZFNet</a></li>
        <li><a href="#34-vggnet">3.4 VGGNet</a>
          <ul>
            <li><a href="#341-概念">3.4.1 概念</a></li>
            <li><a href="#342-vggnet的特点">3.4.2 VGGNet的特点</a></li>
            <li><a href="#343-vggnet的意义">3.4.3 VGGNet的意义</a></li>
          </ul>
        </li>
        <li><a href="#35-googlenetinception-v1">3.5 GoogLeNet/Inception v1</a>
          <ul>
            <li><a href="#351-概念">3.5.1 概念</a></li>
            <li><a href="#352-googlenetinception-v1特点">3.5.2 GoogLeNet/Inception v1特点</a></li>
            <li><a href="#353-从卷积的角度思考如何减少网络中的计算量">3.5.3 从卷积的角度思考，如何减少网络中的计算量？</a></li>
          </ul>
        </li>
        <li><a href="#36-resnet">3.6 ResNet</a>
          <ul>
            <li><a href="#361-概念">3.6.1 概念</a></li>
            <li><a href="#362-resnet中的bootleneck与恒等映射">3.6.2 ResNet中的Bootleneck与恒等映射</a></li>
            <li><a href="#363-resnet中的batchnorm">3.6.3 ResNet中的BatchNorm</a></li>
            <li><a href="#364-resnet的设计特点">3.6.4 ResNet的设计特点</a></li>
            <li><a href="#365-resnet的变种网络">3.6.5 ResNet的变种网络</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#4-轻量型卷积神经网">4 轻量型卷积神经网</a>
      <ul>
        <li><a href="#41-squeezenet">4.1 SqueezeNet</a></li>
        <li><a href="#42-mobilenet-v1v2">4.2 MobileNet V1/V2</a></li>
        <li><a href="#43-shufflenet-v1v2">4.3 ShuffleNet V1/V2</a></li>
        <li><a href="#44-xception">4.4 Xception</a></li>
      </ul>
    </li>
    <li><a href="#5-多分支卷积神经网络">5 多分支卷积神经网络</a>
      <ul>
        <li><a href="#51-siamese-net">5.1 Siamese Net</a></li>
        <li><a href="#52-troplet-net">5.2 Troplet Net</a></li>
        <li><a href="#53-quadruplet-net">5.3 Quadruplet Net</a></li>
        <li><a href="#54-多任务网络">5.4 多任务网络</a></li>
      </ul>
    </li>
    <li><a href="#6-卷积神经网中的attention机制">6 卷积神经网中的Attention机制</a></li>
    <li><a href="#7-模型压缩">7 模型压缩</a>
      <ul>
        <li><a href="#71-模型剪枝">7.1 模型剪枝</a></li>
        <li><a href="#72-模型量化定点化">7.2 模型量化/定点化</a></li>
        <li><a href="#73-知识蒸馏knowledge-distillation">7.3 知识蒸馏（Knowledge Distillation）</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
                </details>
            </div><div class="content" id="content"><p>基于TensorFlow的人脸识别智能小程序的设计与实现 卷积神经网基础串讲</p>
<h2 id="1-卷积神经网发展历程与基本概念">1 卷积神经网发展历程与基本概念</h2>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/1.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h3 id="11-什么是卷积神经网">1.1 什么是卷积神经网</h3>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>以卷积结构为主，搭建起来的深度网络</li>
<li>将图片作为网络的输入，自动提取特征，并对图片的变形（如平移、比例缩放、倾斜）等具有高度不变形</li>
</ul>
</div>
        </div>
<h2 id="2-卷积神经网络的重要组成单元">2 卷积神经网络的重要组成单元</h2>
<h3 id="21-卷积">2.1 卷积</h3>
<h4 id="211-基本定义">2.1.1 基本定义</h4>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>基本定义：对图像和滤波矩阵做内积（逐个元素相乘再求和）的操作
<ul>
<li>滤波器</li>
<li>每一种卷积对应一种特征</li>
<li>lm2col实现卷积运算</li>
</ul>
</li>
</ul>
</div>
        </div>
<h4 id="212-卷集中的重要参数">2.1.2 卷集中的重要参数</h4>
<h5 id="1-卷积核">1. 卷积核</h5>
<ul>
<li>
<p>最常用为2D卷积核<code>（k_w * k_h）</code></p>
</li>
<li>
<p>权重和偏置项</p>
</li>
<li>
<p>常用卷积核：<code>1*1</code>、<code>3*3</code>、<code>5*5</code></p>
<ul>
<li>保护位置信息</li>
<li>padding时对称</li>
</ul>
</li>
</ul>
<h5 id="2-卷积">2. 卷积</h5>
<ul>
<li>权值共享与局部连接（局部感受野/局部感知）</li>
<li>卷积运算作用在局部</li>
<li>Feature map使用同一个卷积核运算后得到一种特征</li>
<li>多种特征采用多个卷积核（channel）</li>
</ul>
<h5 id="3-卷积核与感受野">3. 卷积核与感受野</h5>
<ul>
<li>如何计算卷积参数量？（Parameters）
<ul>
<li><code>( k_w * k_h * In_channel + 1 ) * Out_channel</code></li>
</ul>
</li>
<li>如何计算卷积的计算量？（FLOPs）
<ul>
<li><code>In_w * In_h * ( k_w * k_h * In_channel + 1 ) * Out_channel</code></li>
</ul>
</li>
</ul>
<h5 id="4-步长">4. 步长</h5>
<ul>
<li>下采样过程</li>
<li>输出 Feature Map 的大小如何变化？</li>
<li>参数量和计算量？</li>
</ul>
<h5 id="5-pad">5. Pad</h5>
<ul>
<li>确保Feature Map整数倍变化，对尺度相关的任务尤为重要</li>
<li>参数量和计算量？</li>
</ul>
<h4 id="213-卷积的定义与使用caffe">2.1.3 卷积的定义与使用（caffe）</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">layer</span><span class="p">{</span>
    <span class="n">name</span><span class="p">:</span><span class="s2">&#34;conv1&#34;</span>
    <span class="nb">type</span><span class="p">:</span><span class="s2">&#34;Convolution&#34;</span>
    <span class="n">bottom</span><span class="p">:</span><span class="s2">&#34;data&#34;</span>
    <span class="n">top</span><span class="p">:</span><span class="s2">&#34;conv1&#34;</span>
    <span class="n">param</span><span class="p">{}</span>
    <span class="n">convolution_param</span><span class="p">{}</span>
<span class="p">}</span>

<span class="n">param</span><span class="p">{</span>
    <span class="n">lr_mult</span><span class="p">:</span><span class="mi">1</span> <span class="o">//</span><span class="n">weight</span>
<span class="p">}</span>
<span class="n">param</span><span class="p">{</span>
    <span class="n">lr_mult</span><span class="p">:</span><span class="mi">2</span> <span class="o">//</span><span class="n">bias</span>
<span class="p">}</span>

<span class="n">convlolution_param</span><span class="p">{</span>
    <span class="n">num_output</span><span class="p">:</span><span class="mi">20</span>
    <span class="n">kernel_size</span><span class="p">:</span><span class="mi">5</span>
    <span class="n">stride</span><span class="p">:</span><span class="mi">1</span>
    <span class="n">weight_filler</span><span class="p">{</span>
        <span class="nb">type</span><span class="p">:</span><span class="s2">&#34;xavier&#34;</span><span class="p">}</span>
    <span class="n">bias_filler</span><span class="p">{</span>
        <span class="nb">type</span><span class="p">:</span><span class="s2">&#34;constant&#34;</span><span class="p">}</span>    
    <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="214-卷积的定义与使用tensorflow">2.1.4 卷积的定义与使用（TensorFlow）</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">filter_weight</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">],</span>
<span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">biases</span> <span class="o">=</span> <span class="n">ft</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s1">&#39;biases&#39;</span><span class="p">,[</span><span class="mi">16</span><span class="p">],</span><span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">filter_weight</span><span class="p">,</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span><span class="n">biases</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">slim</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span><span class="mi">64</span><span class="p">,[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="n">scope</span><span class="o">=</span><span class="s1">&#39;conv1_1&#39;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="22-池化">2.2 池化</h3>
<h4 id="221-基本定义">2.2.1 基本定义</h4>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>基本定义：对输入的特征图进行压缩
<ul>
<li>使特征图变小，简化网络计算复杂度</li>
<li>进行特征压缩，提取主要特征</li>
<li>增大感受野</li>
</ul>
</li>
</ul>
</div>
        </div>
<h4 id="222-常见的池化策略">2.2.2 常见的池化策略</h4>
<ul>
<li>最大池化（Max Pooling）</li>
<li>平均池化（Average Pooling）</li>
<li>随机池化（Stochastic Pooling）</li>
</ul>
<h3 id="23-激活">2.3 激活</h3>
<h4 id="231-基本定义">2.3.1 基本定义</h4>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>基本概念：增加网络的非线性，进而提升网络的表达能力
<ul>
<li>非线性</li>
<li>单调性</li>
<li>可微性</li>
<li>取值范围</li>
<li>Sigmoid、Tanh、ReLU、ELU、Maxout、Softplus、Softsign</li>
</ul>
</li>
</ul>
</div>
        </div>
<h4 id="232-sigmoid">2.3.2 Sigmoid</h4>
<ul>
<li>梯度弥散/梯度饱和</li>
<li>指数运算</li>
<li>输出不是以零为中心</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/2.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="233-tanh">2.3.3 Tanh</h4>
<ul>
<li>双曲正切函数（Tanh）</li>
<li>完全可微分的，反对称，对称中心在原点</li>
<li>指数运算</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/3.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="234-relu">2.3.4 ReLU</h4>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/4.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<ul>
<li>修正线性单元（Rectified Linear Unit，ReLU）</li>
<li>保留了step函数的生物学启发（只有输入超出阈值时神经元才激活）</li>
<li>函数形式简单，正数时不存在梯度饱和</li>
<li>一旦输入到了负数，ReLU就会死掉</li>
</ul>
<h3 id="24-bnbatchnorm">2.4 BN（BatchNorm）</h3>
<h4 id="241-基本概念">2.4.1 基本概念</h4>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>基本概念：通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布
<img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/5.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></li>
</ul>
</div>
        </div>
<h4 id="242-batchnorm层优点">2.4.2 BatchNorm层优点</h4>
<ul>
<li>减少了参数的人为选择，可以取消Dropout和L2正则项参数，或者采取更小的L2正则项约束参数；</li>
<li>减少了对学习率的要求</li>
<li>可以不再使用局部响应归一化，BN本身就是归一化网络（局部响应归一化——AlexNet）；</li>
<li>破坏原来的数据分布，一定程度上缓和过拟合。</li>
</ul>
<h4 id="243-batchnorm层使用">2.4.3 BatchNorm层使用</h4>
<p>在Caffe中使用BN层需要注意：</p>
<ul>
<li>要配合Scale层一起使用</li>
<li>训练时use_global_stats设置为 false</li>
<li>测试时user_global_stats设置为 true</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-Python" data-lang="Python"><span class="n">layer</span><span class="p">{</span>
    <span class="n">bottom</span><span class="p">:</span><span class="s2">&#34;conv1&#34;</span>
    <span class="n">top</span><span class="p">:</span><span class="s2">&#34;conv1&#34;</span>
    <span class="n">name</span><span class="p">:</span><span class="s2">&#34;bn_conv1&#34;</span>
    <span class="nb">type</span><span class="p">:</span><span class="s2">&#34;BatchNorm&#34;</span>
    <span class="n">batch_norm_param</span><span class="p">{</span>
        <span class="n">use_global_stats</span><span class="p">:</span><span class="n">true</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="25-全连接层fully-connected-layer">2.5 全连接层（Fully Connected Layer）</h3>
<p>连接所有的特征，将输出值分送给分类器（softmax分类器）</p>
<ul>
<li>将网络的输出变成一个向量</li>
<li>可以采用卷积替代全连接层</li>
<li>全连接层是尺度敏感的</li>
<li>配合使用dropout层</li>
</ul>
<h3 id="26-dropout层">2.6 Dropout层</h3>
<p>在训练过程中，随机的丢弃一部分输入，此时丢弃部分对应的参数不会更新</p>
<ul>
<li>数据过拟合问题
<ul>
<li>取平均的作用</li>
<li>减少神经元之间复杂的共适应关系</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/6.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h3 id="27-损失层loss">2.7 损失层（LOSS）</h3>
<h4 id="271-损失函数">2.7.1 损失函数</h4>
<p>损失函数：用来评估模型的预测值和真实值的不一致程度。</p>
<ul>
<li>经验风险小</li>
<li>结构风险小</li>
<li>交叉熵损失、softmax loss等</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/7.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="272-损失层">2.7.2 损失层</h4>
<p>损失层定义了使用的损失函数，通过最小化损失来驱动网络的训练</p>
<ul>
<li>网络的损失通过前项操作计算</li>
<li>网络参数相对于损失函数的梯度则通过反向操作计算
<ul>
<li>分类任务损失：交叉熵损失</li>
<li>回归任务损失：L1损失、L2损失</li>
</ul>
</li>
</ul>
<h4 id="273-交叉熵损失">2.7.3 交叉熵损失</h4>
<ul>
<li>log-likelihood cost</li>
<li>非负性</li>
<li>当真实输出a与期望输出y接近的时候，代价函数接近于0</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/8.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-Python" data-lang="Python"><span class="c1"># 交叉熵损失实现</span>
<span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entrpoy_with_logits</span><span class="p">(</span><span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entrpoy_with_logits</span><span class="p">(</span><span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>L1、L2、Smooth L1损失
<ul>
<li>Smooth L1是L1的变形，用于Faster RCNN、SSD等网络计算损失
<img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/9.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></li>
</ul>
</li>
</ul>
<h2 id="3-常见的卷积神经网结构">3 常见的卷积神经网结构</h2>
<h3 id="31-lenet">3.1 LeNet</h3>
<p>麻雀虽小五脏俱全</p>
<ul>
<li>1998年，LeCun提出</li>
<li>用于解决手写数字识别，MNIST</li>
</ul>
<h3 id="32-alexnet">3.2 AlexNet</h3>
<h4 id="321-概念">3.2.1 概念</h4>
<ul>
<li>2012年，Hinton的学生Alex Krizhevsky 做出DeepLearning模型。</li>
<li>con - relu - pooling - LRN</li>
<li>fc - relu - dropout</li>
<li>fc - softmax</li>
<li>参数量60M以上</li>
<li>模型大小&gt;200M</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/10.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="322-alexnet的特点">3.2.2 AlexNet的特点</h4>
<ul>
<li>ReLU非线性激活函数</li>
<li>Dropout层防止过拟合</li>
<li>数据增强，减少过拟合</li>
<li>标准化层（Local Response Normalization）</li>
</ul>
<h4 id="323-alexnet的意义">3.2.3 AlexNet的意义</h4>
<ul>
<li>证明了CNN在复杂模型下的有效性</li>
<li>GPU实现使得训练在可接受的时间范围内得到结果</li>
</ul>
<h3 id="33-zfnet">3.3 ZFNet</h3>
<ul>
<li>在AlexNet基础上进行细节调整，并取得2013年ILSVA的冠军
<ul>
<li>从可视化的角度出发，解释CNN有非常好的性能的原因</li>
</ul>
</li>
<li>ZFNet与特征可视化
<ul>
<li>特征分层次体系结构</li>
<li>深层特征更鲁棒</li>
<li>深层特征更有区分度</li>
<li>深层特征收敛更慢</li>
<li>&hellip;&hellip;</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/11.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h3 id="34-vggnet">3.4 VGGNet</h3>
<h4 id="341-概念">3.4.1 概念</h4>
<ul>
<li>由牛津大学计算机视觉组和Google Deepmind共同设计</li>
<li>为了研究网络深度对模型准确度的影响，并采用小卷积堆叠的方式来搭建整个网络结构</li>
<li>参数量：138M</li>
<li>模型大小&gt;500M</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/12.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="342-vggnet的特点">3.4.2 VGGNet的特点</h4>
<ul>
<li>更深的网络结构，结构更加规整、简单；</li>
<li>全部使用3<em>3的小型卷积核和2</em>2的最大池化层</li>
<li>每次池化后Feature Map宽高降低一半，通道数量增加一倍；</li>
<li>网络层数更多、结构更深，模型参数量更大。</li>
</ul>
<h4 id="343-vggnet的意义">3.4.3 VGGNet的意义</h4>
<ul>
<li>证明了更深的网络，能够提取更好的特征</li>
<li>成为后续很多网络的backbone</li>
<li>规范化了后续网络设计的思路</li>
</ul>
<h3 id="35-googlenetinception-v1">3.5 GoogLeNet/Inception v1</h3>
<h4 id="351-概念">3.5.1 概念</h4>
<ul>
<li>在设计网络结构时，不仅强调网络的深度，也会考虑网络的宽度，并将这种结构定义为Inception结构（一种网中网（Network In Network）的结构，及原来的结点也是一个网络）</li>
<li>证明了用更多的卷积，更深的层次可以得到更好的结构</li>
<li>参数量6.8M</li>
<li>模型大小50M</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/13.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h4 id="352-googlenetinception-v1特点">3.5.2 GoogLeNet/Inception v1特点</h4>
<ul>
<li>更深的网络结构</li>
<li>两个LOSS层，降低过拟合风险</li>
<li>考虑网络宽度</li>
<li>巧妙地利用<code>1*1</code>的卷积核来进行通道降维，减小计算量</li>
</ul>
<h4 id="353-从卷积的角度思考如何减少网络中的计算量">3.5.3 从卷积的角度思考，如何减少网络中的计算量？</h4>
<ul>
<li>小卷积核来对大卷积核进行拆分</li>
<li>Stride = 2代替pooling层</li>
<li>巧妙地利用<code>1*1</code>的卷积核来进行</li>
</ul>
<h3 id="36-resnet">3.6 ResNet</h3>
<h4 id="361-概念">3.6.1 概念</h4>
<ul>
<li>2015年，由何凯明团队提出，引入跳连的结构来防止梯度消失的问题，进而可以进一步加大网络深度。</li>
</ul>
<h4 id="362-resnet中的bootleneck与恒等映射">3.6.2 ResNet中的Bootleneck与恒等映射</h4>
<ul>
<li>Bootleneck：跳连结构（Short-Cut）恒等映射，解决梯度消失问题；</li>
</ul>
<h4 id="363-resnet中的batchnorm">3.6.3 ResNet中的BatchNorm</h4>
<ul>
<li>每个卷积之后都会配合一个BatchNorm层</li>
<li>对数据scale和分布来进行约束</li>
<li>简单的正则化，提高网络抗过拟合能力</li>
</ul>
<h4 id="364-resnet的设计特点">3.6.4 ResNet的设计特点</h4>
<ul>
<li>核心单元简单堆叠</li>
<li>跳连结构解决网络梯度消失问题</li>
<li>Average Pooling层代替FC层</li>
<li>BN层加快网络训练速度和收敛时的稳定性</li>
<li>加大网络深度，提高模型特征提取能力</li>
</ul>
<h4 id="365-resnet的变种网络">3.6.5 ResNet的变种网络</h4>
<ul>
<li>ResNetXt	分组卷积</li>
<li>DenseNet	更多的跳连</li>
<li>Wide-ResNet	加大网络宽度</li>
<li>ResNet In ResNet	网中网</li>
<li>Inception-ResNet	Inception结构</li>
</ul>
<h2 id="4-轻量型卷积神经网">4 轻量型卷积神经网</h2>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><ul>
<li>更少的参数量</li>
<li>更少的计算量</li>
<li>移动端、嵌入式平台</li>
</ul>
</div>
        </div>
<h3 id="41-squeezenet">4.1 SqueezeNet</h3>
<ul>
<li>LCLR-2017，作者分别来自Berkeley和Stanford
<ul>
<li>提出 Fire Module，由两部分组成：Squeeze 层 + Expand层</li>
</ul>
</li>
<li>SqueezeNet的特点
<ul>
<li><code>1*1</code>卷积核减小计算量</li>
<li>不同size的卷积核，类似Inception</li>
<li>deep compression技术</li>
</ul>
</li>
</ul>
<h3 id="42-mobilenet-v1v2">4.2 MobileNet V1/V2</h3>
<ul>
<li>由Google团队提出，并发表于CVPR-2017
<ul>
<li>Depth-wise Separable Convolution 的卷积方式代替传统卷积方式，以达到减少网络权值参数的目的。</li>
</ul>
</li>
<li>MobileNet 网络设计思想
<ul>
<li>Depth-wise Convolution</li>
<li>Point-wise Convolution</li>
</ul>
</li>
<li>MobileNet V2
<ul>
<li>Inverted residuals（倒置残差）</li>
<li>Linear bottlenecks</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/14.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h3 id="43-shufflenet-v1v2">4.3 ShuffleNet V1/V2</h3>
<ul>
<li>ShuffleNet V1由旷视科技提出的一种轻量型卷积网络
<ul>
<li>深度卷积来代替标准卷积</li>
<li>分组卷积+通道shuffle</li>
</ul>
</li>
<li>ShuffleNet V2旷视科技针对ShuffleNet V1改进的轻量型卷积神经网
<ul>
<li>ECCV 2018</li>
<li>该模型最大的贡献点在于解释了如何去设计轻量型卷积网络的几个标准和规范</li>
</ul>
</li>
</ul>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><p><strong>轻量型卷积神经网设计标准</strong></p>
<ul>
<li>相同的通道宽度可最小化内存访问成本（MAC）</li>
<li>过度的组卷积会增加MAC</li>
<li>网络碎片化（例如GoogleNet的多路径结构）会降低并行度</li>
<li>元素级运算不可忽视</li>
</ul>
</div>
        </div>
<h3 id="44-xception">4.4 Xception</h3>
<ul>
<li>由Google提出，arXiv的V1版本于2016年10月公开</li>
<li>同样借鉴了深度卷积的思想，但又存在差异，具体如下：</li>
<li>Xception先采用1*1卷积，再进行主通道卷积；</li>
<li>Xception再1*1卷积后，加入ReLU；</li>
</ul>
<h2 id="5-多分支卷积神经网络">5 多分支卷积神经网络</h2>
<h3 id="51-siamese-net">5.1 Siamese Net</h3>
<ul>
<li>孪生网络</li>
<li>余弦距离
<ul>
<li>余弦距离，也称余弦相似度，是用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异大小的度量
<img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/15.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></li>
</ul>
</li>
<li>余弦距离——拓展Loss
<ul>
<li>Center Loss</li>
<li>SphereFace</li>
<li>CosFace</li>
<li>ArcFace</li>
<li>CCL</li>
<li>AMSoftmax</li>
</ul>
</li>
<li>度量问题
<ul>
<li>分类问题</li>
<li>回归问题</li>
<li>度量问题
<ul>
<li>相似度</li>
<li>排序问题</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="52-troplet-net">5.2 Troplet Net</h3>
<ul>
<li>Anchor + Negative + Positive</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/16.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<ul>
<li>Troplet Net 网络结构</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/17.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<ul>
<li>Troplet Net 网络特点
<ul>
<li>提取Embedding Feature</li>
<li>细粒度的识别任务</li>
<li>正负样本比例失衡——难例挖掘</li>
</ul>
</li>
</ul>
<h3 id="53-quadruplet-net">5.3 Quadruplet Net</h3>
<ul>
<li>相比Troplet Net 多加入一张负样本</li>
<li>正度样本之间的绝对距离</li>
</ul>
<h3 id="54-多任务网络">5.4 多任务网络</h3>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/18.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h2 id="6-卷积神经网中的attention机制">6 卷积神经网中的Attention机制</h2>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><p>人类大脑在接受和处理外界信号时的一种机制</p>
<ul>
<li>one-hot分布或者soft的软分布</li>
<li>Soft-Attention或者Hard-Attention</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/19.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
</div>
        </div>
<p>Attention实现机制：</p>
<ul>
<li>
<p>保留所有分量均做加权（即soft attention）</p>
</li>
<li>
<p>在分布中以某种采样策略选取部分分量（即hard attention）</p>
<ul>
<li>原图、特征图、空间尺度、通道、特征图上的每个元素、不同历史时刻</li>
<li>ResNet + Attention</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/20.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<ul>
<li>SENet/Channel Attention</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/21.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
</li>
</ul>
<h2 id="7-模型压缩">7 模型压缩</h2>
<div class="admonition note">
            <p class="admonition-title"><i class="icon fas fa-pencil-alt"></i>笔记</p>
            <div class="admonition-content"><p>学院派VS工程派</p>
<ul>
<li>学院派注重精度</li>
<li>工程派注重精度与效率的结合</li>
</ul>
</div>
        </div>
<h3 id="71-模型剪枝">7.1 模型剪枝</h3>
<ul>
<li>除无意义的权重和激活来减少模型的大小
<ul>
<li>贡献度排序</li>
<li>去除小贡献度单元</li>
<li>重新fine-tuning</li>
</ul>
</li>
<li>模型剪枝技巧
<ul>
<li>全连接部分通常会存在大量的参数冗余</li>
<li>对卷积窗口进行剪枝的方式，可以使减少卷积窗口权重，或者直接丢弃掉卷积窗口的某一维度；</li>
<li>丢弃稀疏的卷积窗口，但这并不会使模型运行速度有数量级的提升；</li>
<li>首先训练一个较大的神经网络模型，在逐步剪枝得到的小模型。</li>
</ul>
</li>
</ul>
<h3 id="72-模型量化定点化">7.2 模型量化/定点化</h3>
<ul>
<li>减少数据在内存中的位数操作，可以采用8位类型来表示32位浮点（定点化）或者直接训练低于8位的模型，比如：2bit模型、4bit模型等
<ul>
<li>较少内存开销，节省更多的带宽</li>
<li>对于某些定点运算方式，甚至可以消除乘法操作，只剩加法操作，某些二值模型，直接使用位操作</li>
</ul>
</li>
<li>代价通常是位数越低，精度下降越明显
<ul>
<li>在TensorFlow中，通常采用引入量化层的方式来更改计算图，进而达到量化的目的。</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/22.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p>
<h3 id="73-知识蒸馏knowledge-distillation">7.3 知识蒸馏（Knowledge Distillation）</h3>
<p>采用一个大的、复杂的网络模型来指导一个小的、精简之后的网络模型进行模型训练和学习</p>
<p><img
        class="lazyload"
        src="/svg/loading.small.min.svg"
        data-sizes="auto"
        data-srcset="/images/face/face03/23.jpg,  1.5x,  2x"
        data-src=""
        alt="Minion"
        title="Minion" /></p></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>本文于 2020-03-12 更新</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲" data-via="ieblYang" data-hashtags="人脸识别,卷积神经网"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://ieblyang.tech/face03/" data-hashtag="人脸识别"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Linkedin" data-sharer="linkedin" data-url="https://ieblyang.tech/face03/"><i class="fab fa-linkedin fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲" data-image="/images/face/featured-image.png" data-ralateuid="ieblYang"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 百度" data-sharer="baidu" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲"><i class="loveit it-baidu-fill"></i></a><a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲"><i class="fab fa-evernote fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Skype" data-sharer="skype" data-url="https://ieblyang.tech/face03/" data-title="卷积神经网基础串讲"><i class="fab fa-skype fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB">人脸识别</a>,&nbsp;<a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91">卷积神经网</a></section>
        <section>
            <span><a href="javascript:window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/face02/" class="prev" rel="prev" title="深度学习基础串讲"><i class="fas fa-angle-left fa-fw"></i>深度学习基础串讲</a>
            <a href="/face04/" class="next" rel="next" title="TensorFlow基础串讲">TensorFlow基础串讲<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div class="comment"><div id="valine"></div><script>
            document.addEventListener("DOMContentLoaded", function(event) {
                new Valine({
                    el: '#valine',
                    appId: 'kpgPchgfg6K9RMdLzdhxQyOV-gzGzoHsz',
                    appKey: 'erpDYbf9JmO61Yn1YtUw7AXB',placeholder: '你的评论 ...',verify: true,avatar: 'mp',pageSize: 10,lang: 'zh-cn',visitor: true,recordIP: true,});
            });
        </script>
        <noscript>
            Please enable JavaScript to view the <a href="https://valine.js.org/">comments powered by Valine.</a>
        </noscript></div>
    </article></div>
            </main><footer class="footer">
    <div class="copyright"><div class="copyright-line">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="external nofollow noopener noreffer"><i class="far fa-heart fa-fw"></i> LoveIt</a>
        </div>

        <div class="copyright-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">ieblYang</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
    </div>
</footer>
</div><a href="#" class="dynamic-to-top animated faster" id="dynamic-to-top">
            <i class="fas fa-chevron-up fa-fw"></i>
        </a><script>
        document.addEventListener('DOMContentLoaded', function () {
            lightGallery(document.getElementById('content'), {
                selector: '.lightgallery',
                speed: 400,
                hideBarsDelay: 2000,
                thumbnail: true,
                exThumbImage: 'data-thumbnail',
                thumbWidth: 80,
                thumbContHeight: 80,
            });
        });
    </script><link rel="stylesheet" href="/lib/valine/valine.48fc8c7a134ff56a24e2e2d3106ec39a.css" integrity="md5-SPyMehNP9Wok4uLTEG7Dmg=="><link rel="stylesheet" href="/lib/iconfont/iconfont.min.60491d359fba28667d44b1b4c808a0bd.css" integrity="md5-YEkdNZ&#43;6KGZ9RLG0yAigvQ=="><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.f0e116a1a6b325059928a106254352e1.css" integrity="md5-8OEWoaazJQWZKKEGJUNS4Q=="><script src="/lib/valine/Valine.min.1698240d38b9277036a3059775e55a03.js" integrity="md5-FpgkDTi5J3A2owWXdeVaAw=="></script><script src="/lib/smooth-scroll/smooth-scroll.polyfills.min.c4f5974788ebb82890a0e7771763939c.js" integrity="md5-xPWXR4jruCiQoOd3F2OTnA=="></script><script src="/lib/sharer/sharer.min.c3c3372b4cf5c56dd4e5a4be8ada86c9.js" integrity="md5-w8M3K0z1xW3U5aS&#43;itqGyQ=="></script><script src="/lib/lazysizes/lazysizes.min.0812d0f17b90a4aefd97bb91085ad252.js" integrity="md5-CBLQ8XuQpK79l7uRCFrSUg=="></script><script src="/lib/lightgallery/lightgallery.min.4bc95325ef47ddceda10ae0ee66e925c.js" integrity="md5-S8lTJe9H3c7aEK4O5m6SXA=="></script><script src="/lib/lightgallery/lg-thumbnail.min.874045ba82acdc496e8c871b77c5c1e5.js" integrity="md5-h0BFuoKs3ElujIcbd8XB5Q=="></script><script src="/lib/lightgallery/lg-zoom.min.409e9d2da92a979799deed01377eeff6.js" integrity="md5-QJ6dLakql5eZ3u0BN37v9g=="></script><script src="/js/theme.min.2d8c56f0eeaac5b7d58911d3c7522b29.js" integrity="md5-LYxW8O6qxbfViRHTx1IrKQ=="></script></body>
</html>
